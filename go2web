#!/usr/bin/env python3
import argparse
from bs4 import BeautifulSoup
import hashlib
import html2text
from pathlib import Path
import pickle
import socket
import ssl
from urllib.parse import urlparse, quote_plus
import webbrowser

CACHE_FOLDER = Path.home() / '.go2web_cache'
CACHE_FOLDER.mkdir(exist_ok=True)


def generate_url_hash(url_string: str) -> str:
    return hashlib.md5(url_string.encode()).hexdigest()


def load_cached_data(url_string: str) -> str | None:
    url_hash = generate_url_hash(url_string)
    cache_path = CACHE_FOLDER / url_hash
    if cache_path.exists():
        with open(cache_path, 'rb') as cache_file:
            return pickle.load(cache_file)
    return None


def store_in_cache(url_string: str, response_data: str) -> None:
    url_hash = generate_url_hash(url_string)
    cache_path = CACHE_FOLDER / url_hash
    with open(cache_path, 'wb') as cache_file:
        pickle.dump(response_data, cache_file)


def fetch_web_content(target_url, accept_header='text/html', remaining_redirects=10):
    if remaining_redirects <= 0:
        return None, "Redirect limit exceeded"

    url_hash = generate_url_hash(target_url)
    cached_content = load_cached_data(url_hash)
    if cached_content:
        return cached_content['content_type'], cached_content['body']

    try:
        parsed_url = urlparse(target_url)
        if not parsed_url.scheme:
            target_url = 'http://' + target_url
            parsed_url = urlparse(target_url)

        domain = parsed_url.netloc
        url_path = parsed_url.path or '/'
        if parsed_url.query:
            url_path += '?' + parsed_url.query

        request_headers = {
            'User-Agent': 'Mozilla/5.0',
            'Accept': accept_header,
            'Connection': 'close',
            'Host': domain
        }

        http_request = f"GET {url_path} HTTP/1.1\r\n"
        http_request += '\r\n'.join(f'{key}: {value}' for key, value in request_headers.items())
        http_request += '\r\n\r\n'

        ssl_context = ssl.create_default_context()
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as connection:
            if parsed_url.scheme == 'https':
                connection = ssl_context.wrap_socket(connection, server_hostname=domain)
            connection_port = 443 if parsed_url.scheme == 'https' else 80
            connection.connect((domain, connection_port))
            connection.sendall(http_request.encode())

            raw_response = b''
            while True:
                chunk = connection.recv(4096)
                if not chunk:
                    break
                raw_response += chunk

        header_data, _, content = raw_response.partition(b'\r\n\r\n')
        headers = header_data.decode('utf-8', errors='ignore')

        first_line = headers.split('\r\n')[0]
        if '301' in first_line or '302' in first_line:
            for header_line in headers.split('\r\n'):
                if header_line.lower().startswith('location:'):
                    redirect_url = header_line.split(':', 1)[1].strip()
                    if not redirect_url.startswith('http'):
                        redirect_url = f"{parsed_url.scheme}://{domain}{redirect_url}"
                    return fetch_web_content(redirect_url, accept_header, remaining_redirects - 1)

        response_type = 'text/html'
        for header_line in headers.split('\r\n'):
            if header_line.lower().startswith('content-type:'):
                response_type = header_line.split(':', 1)[1].strip()
                break

        text_content = content.decode('utf-8', errors='ignore')

        store_in_cache(url_hash, {
            'content_type': response_type,
            'body': text_content
        })

        return response_type, text_content

    except Exception as error:
        print(f"Request error: {str(error)}")
        return None, None


def format_content(response_type, content_body):
    if 'application/json' in response_type:
        try:
            import json
            parsed_json = json.loads(content_body)
            return json.dumps(parsed_json, indent=2)
        except:
            return content_body  # Return raw content if JSON parsing fails
    else:
        return convert_html_to_text(content_body)


def convert_html_to_text(html_content):
    converter = html2text.HTML2Text()
    converter.ignore_links = False
    converter.ignore_images = True
    return converter.handle(html_content)


def perform_search(search_query):
    try:
        search_url = f"http://www.bing.com/search?q={quote_plus(search_query)}"
        content_type, html_data = fetch_web_content(search_url)
        if not html_data:
            return []

        page_soup = BeautifulSoup(html_data, 'html.parser')
        search_results = []

        for item in page_soup.find_all('li', class_='b_algo'):
            result_link = item.find('a')
            if result_link:
                result_title = result_link.get_text(strip=False)
                result_url = result_link['href']
                search_results.append({
                    'title': result_title,
                    'link': result_url
                })
                if len(search_results) >= 10:
                    break

        print(f"Top {len(search_results)} results for '{search_query}':\n")
        for index, result in enumerate(search_results, 1):
            print(f"{index}. {result['title']}")
            print(f"   {result['link']}\n")

        try:
            user_choice = int(input("Enter result number to open (0 to skip): "))
            if 1 <= user_choice <= len(search_results):
                webbrowser.open(search_results[user_choice - 1]['link'])
                print("Opening in default browser...")
            elif user_choice == 0:
                print("No selection made.")
            else:
                print("Invalid selection.")
        except ValueError:
            print("Please enter a valid number.")

        return search_results

    except Exception as error:
        print(f"Search failed: {str(error)}")
        return []


def execute_cli():
    arg_parser = argparse.ArgumentParser(description='go2web - Web client utility')
    arg_parser.add_argument('-u', '--url', help='Make an HTTP request to the specified URL and print the response')
    arg_parser.add_argument('-s', '--search', nargs='+', help='Make an HTTP request to search the term using your favorite search engine and print top 10 results')
    arg_parser.add_argument('--json', action='store_true', help='Request JSON format')
    arguments = arg_parser.parse_args()

    if not any(vars(arguments).values()):
        arg_parser.print_help()
        return

    if arguments.url:
        accept_format = 'application/json' if arguments.json else 'text/html'
        content_type, web_content = fetch_web_content(arguments.url, accept_format)
        if web_content:
            print(format_content(content_type, web_content))
    elif arguments.search:
        query = ' '.join(arguments.search)
        results = perform_search(query)

        if results:
            print(f"Top {len(results)} results for '{query}':\n")
            for idx, item in enumerate(results, 1):
                print(f"{idx}. {item['title']}")
                print(f"   {item['link']}\n")
        else:
            print("No results found. Try different terms.")


if __name__ == '__main__':
    execute_cli()